import os
import re
import yaml
import requests
import collections
from .config import get_index as get_config_index
from .config import get_release_build_files
from .status_page import REPOS_DATA_NAMES as APT_REPOS

"""
Aggregate Status takes the yaml generated by the different CPU architectures (e.g. ros_lunar_default, ros_lunar_ds)
and combines them into one yaml file per ROS distro.
It also creates an English description of each package across all of the architectures.

Example Descriptions:
 - released - All apt_repos have the same version number
 - waiting for new release - Build/test have the same version number, but there's nothing for main
 - waiting for re-release - Build/test have the same version number, but there's an older version for main
 - waiting for new/re-release - Build/test have the same version number, but there's either nothing or an older
                                version for main
 - source builds, binary doesn't - If the source builds, but all the other architectures do not
 - does not build on X - If the source and some of the architectures build, but some do not
"""

YAML_FOLDER = 'http://repositories.ros.org/status_page/yaml/'
YAML_PATTERN = re.compile('<a href="(ros_(\w+)_(\w+).yaml)">')
VERSION_PATTERN = re.compile('([\d\-\.]+)\w+.*')


def get_yaml_filenames(rosdistro_key=None):
    """
    Scrape the YAML_FOLDER for filenames matching the YAML_PATTERN and return those with the matching rosdistro
    """
    filenames = {}
    r = requests.get(YAML_FOLDER)
    for filename, rosdistro, build_file_name in YAML_PATTERN.findall(r.text):
        if rosdistro_key and rosdistro != rosdistro_key:
            continue
        filenames[rosdistro, build_file_name] = filename
    return filenames


def _dict_merge(dct, merge_dct):
    """
    Recursively merge merge_dct into dct
    """
    for k, v in merge_dct.items():
        if (k in dct and isinstance(dct[k], dict) and isinstance(merge_dct[k], collections.Mapping)):
            _dict_merge(dct[k], merge_dct[k])
        else:
            dct[k] = merge_dct[k]


def get_aggregate_status(rosdistro):
    """
    Load all of the individual yaml files and merge them into one giant dictionary.
    """
    status = {}
    for (rosdistro, build_file_name), filename in sorted(get_yaml_filenames(rosdistro).items()):
        print('  Loading {}/{}'.format(rosdistro, build_file_name))
        r = requests.get(YAML_FOLDER + filename)
        rosdistro_status = yaml.load(r.text)
        _dict_merge(status, rosdistro_status)
    return status


def _get_blacklist(build_file):
    """
    Load the package blacklist from the build_file.
    Returns a dictionary mapping the package name to a set of tuples
    (os_name, os_code_name, arch) that are blacklisted
    """
    blacklist = collections.defaultdict(set)
    for build_file_name in build_file:
        build_config = build_file[build_file_name]
        if len(build_config.package_blacklist) == 0:
            continue
        for pkg in build_config.package_blacklist:
            for os_name, os_d in build_config.targets.items():
                for os_code_name, osc_d in os_d.items():
                    for arch in osc_d:
                        blacklist[pkg].add((os_name, os_code_name, arch))
    return dict(blacklist)


def _get_expected_architectures(build_file):
    """
    Collect the expected CPU architectures, sorted by os_name and os_code_name.
    Returns a dictionary of dictionaries of sets, where
     * the first key is a os_name
     * the second key is a os_code_name
     * the set values are architectures
    """
    architectures = collections.defaultdict(lambda: collections.defaultdict(set))
    for build_file_name in build_file:
        build_config = build_file[build_file_name]
        for os_name, os_d in build_config.targets.items():
            for os_code_name, osc_d in os_d.items():
                architectures[os_name][os_code_name].add('source')
                for arch in osc_d:
                    architectures[os_name][os_code_name].add(arch)
    return architectures


def _some_other_value_matches(m, value_list, exclude):
    """
    Returns true if some value in m except the one with key=exclude
    matches the set version of the list of values passed in.
    """
    values = set(value_list)
    for key in m.keys():
        if key != exclude and m[key] == values:
            return True
    return False


def _no_overlap_in_values_and_none(m):
    if None not in m:
        return False
    # Assume m has two key/value pairs
    values0, values1 = m.values()
    return len(values0.intersection(values1)) == 0


def get_status_description(build_status, expected, blacklist, apt_repos=APT_REPOS, skip_source=False, debug=False):
    """
    build_status is a recursive structure that maps a specific build i.e. (os_name, os_code_name, arch, apt_repo)
    to a version number.
    If the version number is not present, then it is an error, i.e. version_number = None

    The expected parameter is data structure returned by _get_expected_architectures (constant across all packages)

    The blacklist parameter is a set of (os_name, os_code_name, arch) that we don't expect to see
    (specific to this package)
    """

    # The first thing we do is reverse the mapping
    # version_map maps version to a list of (os_name, os_code_name, arch, apt_repo)
    version_map = collections.defaultdict(list)

    # the following four structures map the version to either the os_name, os_code_name, etc
    os_map = collections.defaultdict(set)
    code_name_map = collections.defaultdict(set)
    arch_map = collections.defaultdict(set)
    apt_repo_map = collections.defaultdict(set)

    # this maps the version to the os_code_name + arch
    target_map = collections.defaultdict(set)

    # build the reversed maps
    for os_name in expected:
        os_d = build_status.get(os_name, {})
        for os_code_name in expected[os_name]:
            osc_d = os_d.get(os_code_name, {})
            for arch in expected[os_name][os_code_name]:
                if skip_source and arch == 'source':
                    continue
                arch_d = osc_d.get(arch, {})
                for apt_repo in apt_repos:
                    version = None
                    if apt_repo in arch_d:
                        version = VERSION_PATTERN.match(arch_d[apt_repo]).group(1)
                    elif (os_name, os_code_name, arch) in blacklist:
                        continue
                    os_map[version].add(os_name)
                    code_name_map[version].add(os_code_name)
                    arch_map[version].add(arch)
                    apt_repo_map[version].add(apt_repo)
                    target_map[version].add(os_code_name + '/' + arch)
                    version_map[version].append((os_name, os_code_name, arch, apt_repo))

    # If there's only one version across all builds (and nothing is missing), then
    # this package is completely synced and released
    if len(version_map) == 1:
        return 'released'

    # If there are two different versions available
    if len(version_map) == 2:
        if apt_repo_map[None] == set(['main']):
            # one version for build/test, and all the main builds are None
            # this package hasn't been released yet, but it builds fine otherwise
            return 'waiting for new release'
        elif set(['main']) in apt_repo_map.values():
            # still one version for build/test, and some other version for main
            return 'waiting for re-release'

        if set(['source']) in arch_map.values():
            if None in arch_map:
                return 'source builds, binary doesn\'t'

            # old_version, new_version
            arch_version0, arch_version1 = sorted(arch_map.keys())
            if arch_map[arch_version0] == set(['source']):
                # the source is the only thing that builds for the new version
                return 'source builds, binary doesn\'t'

        if _no_overlap_in_values_and_none(code_name_map):
            # if the thing that separates the working builds and nonworking builds is the os_code_name
            return 'does not build on ' + ', '.join(sorted(code_name_map[None]))
        elif _no_overlap_in_values_and_none(arch_map):
            return 'does not build on ' + ', '.join(sorted(arch_map[None]))
        elif _no_overlap_in_values_and_none(target_map):
            return 'does not build on ' + ', '.join(sorted(target_map[None]))

    # If there are three different versions availble
    elif len(version_map) == 3:
        if apt_repo_map[None] == set(['main']) and \
           _some_other_value_matches(apt_repo_map, ['main'], None):
            # This package builds fine in build/test, and has two different versions
            # in main, including None, then been released for some builds but not others
            return 'waiting for new/re-release'

    if None in version_map:
        # if some version is breaking (and doesn't match the above patterns)
        # gather all the code_name/arch combos that are breaking
        broken_set = set([(os_code_name, arch) for os_name, os_code_name, arch, apt_repo in version_map[None]])
        if len(broken_set) == 1:
            os_code_name, arch = list(broken_set)[0]
            return 'does not build on %s/%s' % (os_code_name, arch)

    if apt_repos == APT_REPOS:
        sub_apt_repos = APT_REPOS[:-1]  # skip main
        status = get_status_description(build_status, expected, blacklist, sub_apt_repos)
        if status and status != 'released':
            return status

        status = get_status_description(build_status, expected, blacklist, sub_apt_repos, True)
        if status and status != 'released':
            return 'binary: ' + status

    if not debug:
        return
    print('{} versions ({})'.format(len(version_map), ', '.join(map(str, sorted(version_map)))))
    for name, m in [('os', os_map), ('code_name', code_name_map), ('arch', arch_map), ('apt_repo', apt_repo_map),
                    ('target', target_map)]:
        print(name)
        for version, values in m.items():
            print('\t{}: {}'.format(version, ', '.join(values)))

    for version, m in sorted(version_map.items()):
        print(version, len(m))
        if version is None:
            print(m)
    print()


def build_aggregate_status(config_url, rosdistro, output_dir='.'):
    print('Merging yaml files')
    aggregate_status = get_aggregate_status(rosdistro)
    print('Classify statuses')
    config = get_config_index(config_url)
    build_file = get_release_build_files(config, rosdistro)
    expected = _get_expected_architectures(build_file)
    blacklist = _get_blacklist(build_file)
    for pkg, entry in aggregate_status.items():
        description = get_status_description(entry['build_status'], expected, blacklist.get(pkg, set()))
        entry['description'] = description

    print('Write yaml file')
    yaml_filename = os.path.join(output_dir, '{}.yaml'.format(rosdistro))
    with open(yaml_filename, 'w') as f:
        yaml.safe_dump(aggregate_status, f, allow_unicode=True)
